{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b53544",
   "metadata": {},
   "source": [
    " ## Dimensionality reduction\n",
    " \n",
    "Dimensionality reduction is a method to represent a given dataset with lesser number of features (dimensions) that it originaly had. This provides advantages for clustering, classification and in general interpret  data.\n",
    "\n",
    "In this section we will use an example taken from petrophysics where the task is to perform simple depth alignment of borehole logs. The examples are taken from a research paper:\n",
    "[S. Acharya and K. Fabian, 2024](https://asmedigitalcollection.asme.org/OMAE/proceedings/OMAE2024/87868/V008T11A019/1202880)\n",
    "The problem is illustrated by the figure below which shows the same log type, but measured at different times and with different systems resulting in a misalignment in depth.\n",
    "![Log depth misalignment](clusterlogs.png)\n",
    "\n",
    "# The data set\n",
    "The data set we are going to use consists of four different well logs; Bulk density, Resistivity, gamma ray and neutron porosity.\n",
    "Before application of machine learning each of the logs are standarized to have a mean value of 0 and a standard deviation of 1, as shown in the figure below.\n",
    "\n",
    "![Log normalization](dim-norm.png)\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "\n",
    "The log dataset is organized into a matrix $\\mathbf{X}$ where each of four column contains\n",
    "the (normalized) log measurements. The number of measurements is of the order of $N=10^5$ and there is a row for each measurement. The dimension of $\\mathbf{X}$ is thus \n",
    "$N \\times 4$.\n",
    "The $mathbf{X}$ matrix can be factorized into three separate matrices using so-called\n",
    "SVD decomposition\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{W}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here $\\mathbf{\\Sigma}$ is an $N \\times 4$ matrix where the diagonal contains the so-called\n",
    "singular values of $\\mathbf{X}$. $\\mathbf{W}$ is a $4 \\times 4 $ matrix whose columns contains orthogonal unit vectors, while $\\mathbf{U}$ is an $N\\times N$ matrix whos columns are ortogonal unit vectors.\n",
    "We can now define a new transformed data matrix by the relation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{T} = \\mathbf{X} \\mathbf{W} = \\mathbf{U}\\mathbf{\\Sigma}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The $\\mathbf{T}$ is a projection of the original data onto the orthogonal unit vectors\n",
    "defined by the columns of $\\mathbf{W}$.\n",
    "\n",
    "# Dimensionality reduction\n",
    "The $\\mathbf{T}$ can be approximated with a truncated matrix $\\mathbf{T}_L$ by using\n",
    "only the $L < 4 $ largest singular values of $\\mathbf{\\Sigma}$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{T}_L = \\mathbf{X} \\mathbf{W}_L = \\mathbf{U}_L\\mathbf{\\Sigma}_L\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here $\\mathbf{W}_L$ is an $N\\times L$ matrix. The first column of the truncated matrix \n",
    "$\\mathbf{T}$ corresponds to the so-called\n",
    "first principal value of $\\mathbf{X}$. The second principal component would correspond to the second column\n",
    "of $\\mathbf{T}$ while the third and fourth principal component are given by the third and fourth column.\n",
    "\n",
    "The principal component has the property that the transformed data set includes the data in the original dataset with the maximum variance. So that the first component maps data with larger variance than the second, third and fourth component. The idea is then that the datapoints with the largest variance contains the largest amount of information. In the figure below the variance of the four principal components are plotted.\n",
    "\n",
    "![Variance of principal components](dim-var.png)\n",
    "\n",
    "We see that the two first columns contains 80$\\%$ of the variance. The dataset can then be reduced from a\n",
    "dimension of $N\\times 4$ to $N\\times 2$. \n",
    "\n",
    "To identify the layers the next step would be to use the kmeans clustering algorithm on the transformed and\n",
    "truncated dataset $\\mathbf{T}_L$.\n",
    "\n",
    "The result of the procedure is shown in the figure below.\n",
    "\n",
    "![pca](dim-pca.png)\n",
    "\n",
    "Below is the result of using the kmeans clustering algorithm on the full dataset.\n",
    "Note that the pca based method detects an extra layer.\n",
    "\n",
    "![kmeans](dim-kmeans.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73425c8",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "An autoencoder works in a similar way as the pca dimensionality reduction. It is a neural network which learns to encode a data vector into a representation with lower dimensionality. The network also have a decode stage which uses the encoded data set to reconstruct the original dataset, i.e. it increases the dimensionality.\n",
    "The figure below shows the layout of the network.\n",
    "\n",
    "![Autodecoder](dim-autoencoder.png)\n",
    "\n",
    "The network is trained by using a loss function which minimizes the error between an input vector $\\mathbf{x}$\n",
    "and the output vector $\\hat{\\mathbf{x}}$ :\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  $\\mbox{min}\\, ||\\mathbf{x}-\\hat{\\mathbf{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can use an autoencoder network to perform dimensionality reduction on our log dataset, as shown in the figure below.\n",
    "\n",
    "![Autodecoder](dim-autolog.png)\n",
    "\n",
    "Here we reduce the dimensionality of the log dataset from four into two. The figure below shows the training error\n",
    "and the estimation error for the log data sets as function of the number of epochs.\n",
    "\n",
    "![Autodecoder](dim-err.png)\n",
    "\n",
    "The error function is reduced by approximately 50 $\\%$ over 100 epochs, which is not fantastically good, but tolerable.\n",
    "\n",
    "The output from the middle layer of the autoencoder gives a new transformed dataset with two dimensions  which is used as an input for the k-means clustering algorithm.\n",
    "The result is shown in the figure below\n",
    "\n",
    "![Autodecoder](dim-autocluster.png)\n",
    "\n",
    "Comparing this with the pca based clustering we see that the autoencoder essentially gives the same results.\n",
    "\n",
    "Both the pca based clustering and the autoencoder based clustering performs better than the clustering using the k-means algorithm on the original (not dimensionality reduced) dataset. A possible explanation for this is that reducing the dimension of the input data set also removes noise from the data. This is shown in the table below\n",
    "which summarises clustering errors for the different algorithms.\n",
    "\n",
    "![Autodecoder](dim-res3.png)\n",
    "\n",
    "Finaly the table below shows a summary of the effectivness of the three different algorithms used in the study.\n",
    "\n",
    "![Autodecoder](dim-res4.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
