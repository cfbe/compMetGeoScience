{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406a2aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8d538",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Automatic differentiation is a tecnique for computing *exact* numerical derivatives of a simple expression or complicated models. The main idea is to decompose any numerical expression, simple or complicated', into a series of elementary steps where each step can be easily differentiated.\n",
    "\n",
    "A common task in optimization is to compute the derivative of an object function. In many cases the object function is a single number but may depend on many variables, and the relation between the variables and the function value might be such that the derivatives in principle can be computed, but might take a lot of effort. In such cases Automatic Differentiation can be used. \n",
    "\n",
    "We will first illustrate the method with an (over) simplified example. Assume that we want to differentiate the function\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  f(x,y) = x^2 + y^2\n",
    "                  \\label{eq:f}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can, of course, compute the derivatives of this function with respect to $x$ and $y$ trivialy as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial f(x,y)}{\\partial x} = 2x, \\nonumber\\\\\n",
    "  \\frac{\\partial f(x,y)}{\\partial y} = 2y.\n",
    "\\end{eqnarray} \n",
    "\n",
    "Alternatively we can first split the computation of $f(x,y)$ into three simple steps\n",
    " \n",
    "1. $w_1  =  x^2$ \n",
    "2. $w_2  =  y^2$\n",
    "3. $w_3  = w_1 + w_2$\n",
    "\n",
    "\n",
    "Formaly $w_3$ is a function of $w_2$ and $w_1$ which again is a function of $x$ and $y$, so to compute the partial derivatives of $w_3 = f$ we have to use the chain rule\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w_3}{\\partial x} & = & \\frac{\\partial w_3}{\\partial w_1}\\frac{\\partial w_1}{\\partial x},\n",
    "                                \\nonumber \\\\\n",
    "\\frac{\\partial w_3}{\\partial y} & = &\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial y}.\n",
    "                                \\label{eq:chain}\n",
    "\\end{eqnarray}\n",
    "We see that each of the partial derivatives in the chain rule can be computed as:\n",
    "1. $\\frac{\\partial w_1}{\\partial x} = 2x$\n",
    "2. $\\frac{\\partial w_2}{\\partial y} = 2y$\n",
    "3. $\\frac{\\partial w_3}{\\partial w_1} = 1$\n",
    "4. $\\frac{\\partial  w_3}{\\partial w_2} = 1$\n",
    "\n",
    "\n",
    "To compute the derivatives we use the list above and add the computation of the derivatives to\n",
    "each step\n",
    "\n",
    "1. $w_1  =  x^2$, $\\frac{\\partial w_1}{\\partial x}=2x$\n",
    "2. $w_2  =  y^2$, $\\frac{\\partial w_2}{\\partial y}=2y$\n",
    "3. $w_3  = w_1 + w_2$, $\\frac{\\partial w_3}{\\partial w_1}\\frac{\\partial w_1}{\\partial x} \n",
    "                       +\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial y}\n",
    "                       = 1\\cdot 2x + 1\\cdot 2y. =2x+2y$\n",
    "                       \n",
    " We see that in the evaluation of the derivatives we start with the derivatives of $w_1$ and $w_2$ with respect to $x$ and $y$ and then continue with the derivatives of $w_3$ with respect to $w_1$ and $w_2$.\n",
    "Refering to equation (\\ref{eq:chain}) we proceed from *right* to *left*.\n",
    "\n",
    "How can we use these concepts to do actual computations?\n",
    "\n",
    "A simple implementation in python is shown below.\n",
    "\n",
    "                      \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c40453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:   2\n",
      "df:  4\n"
     ]
    }
   ],
   "source": [
    "''' Automatic differentiation of simple expression'''\n",
    "\n",
    "def f1(x):\n",
    "  ''' Compute derivative of x*x\n",
    "       \n",
    "    Arg   : x input argument\n",
    "    Return: list of function value and derivative\n",
    "  ''' \n",
    "  return [x*x,2*x]\n",
    "\n",
    "def f2(y):\n",
    "  ''' Compute derivative of x*x\n",
    "       \n",
    "    Arg   : x input argument\n",
    "    Return: list of function value and derivative\n",
    "  '''\n",
    "  return [y*y, 2*y]\n",
    "\n",
    "def f3(w1,w2):\n",
    "  ''' Compute derivative of w1+w1\n",
    "       \n",
    "    Args   : w1,w2 list with function value and derivative\n",
    "    Return: list of function value and total derivative\n",
    "  '''\n",
    "  fval = w1[0]+w2[0]\n",
    "  dval = 1*w1[1]+1*w2[1]\n",
    "  return [fval,dval]\n",
    "\n",
    "#Initial values for x and y\n",
    "x=1\n",
    "y=1\n",
    "\n",
    "# Step 1\n",
    "w1 = f1(x)\n",
    "# Step 2\n",
    "w2 = f2(y)\n",
    "# Step 3\n",
    "w3 = f3(w1,w2)\n",
    "print(\"f:  \",w3[0])\n",
    "print(\"df: \",w3[1])\n",
    "  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5655a",
   "metadata": {},
   "source": [
    "In the above equations and python code note that what we actually compute is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\delta f = \\frac{\\partial f(x,y)}{\\partial x} + \\frac{\\partial f(x,y)}{\\partial y},\n",
    "                                                          \\label{eq:df}\n",
    "\\end{eqnarray}\n",
    "\n",
    "which is the total derivative of $f$. If we instead want the components of the gradient we need\n",
    "the vector\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\nabla f(x,y) = \\left[\\frac{\\partial f(x,y)}{\\partial x},\\frac{\\partial f(x,y)}{\\partial y}\\right] \\nonumber \n",
    "\\end{eqnarray}\n",
    "\n",
    "What we have computed is formaly \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\delta f = \\nabla f(x,y)\\cdot \\mathbf{e}\n",
    "\\end{eqnarray}\n",
    "\n",
    "which is  the dot product of the gradient with respect to the unit vector $\\mathbf e=(1,1)$.\n",
    "To actually compute the gradient we would have to modify the steps above as\n",
    "\n",
    "1. $w_1  =  x^2$, $\\frac{\\partial w_1}{\\partial x}=2x$\n",
    "2. $w_2  =  y^2$, $\\frac{\\partial w_2}{\\partial y}=2y$\n",
    "3. $w_3  = w_1 + w_2$, $\\frac{\\partial w_3}{\\partial w_1}\\frac{\\partial w_1}{\\partial x}e_x \n",
    "                       +\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial y}e_y\n",
    "                       = 1\\cdot 2x \\cdot e_x + 1\\cdot 2y \\cdot e_y =2x+2y$\n",
    " where $e_x$ and $e_y$ are the $x$ and $y$ components of the unit vector $\\mathbf{e} = (e_x,e_y)$ \n",
    " If we consider the unit vector $\\mathbf{e}$ as input to the computation we can modify the python code as \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe96b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:   2\n",
      "df/dx:  2\n",
      "df/dy:  2\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "  return [x*x,2*x]\n",
    "\n",
    "\n",
    "def f3(w1,w2,e):\n",
    "    fval = w1[0]+w2[0]\n",
    "    dval = e[0]*w1[1]+e[1]*w2[1]\n",
    "    return [fval,dval]\n",
    "\n",
    "\n",
    "\n",
    "def df(x,e):\n",
    "  # Step 1\n",
    "  w1 = f1(x[0])\n",
    "  # Step 2\n",
    "  w2 = f1(x[1])\n",
    "  # Step 3\n",
    "  w3 = f3(w1,w2,e)\n",
    "  return w3\n",
    "\n",
    "#Compute x-component of gradient\n",
    "x=[1,1]\n",
    "e=[1,0]\n",
    "f=df(x,e)\n",
    "print(\"f:  \",f[0])\n",
    "print(\"df/dx: \",f[1])\n",
    "\n",
    "#Compute y-component of gradient\n",
    "e=[0,1]\n",
    "f=df(x,e)\n",
    "print(\"df/dy: \",f[1])\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588da428",
   "metadata": {},
   "source": [
    "In effect we have to evaluate the function $f$ twice to get the gradient components. If $f$ have a large number of components we have to evaluate the function many times.\n",
    "\n",
    "## The backward method\n",
    "\n",
    "The method to compute derivatives outlined above uses the chain rule. Each step consist \n",
    "of a multiplication of the derivatives from previous steps and the derivative in the current step. Refering\n",
    "to the chain rule we evaluate the partial derivatives right to left starting with\n",
    "$\\frac{\\partial w_1}{\\partial x}$. In principle we could also compute the derivatives in the opposite\n",
    "order.\n",
    "Starting with $w_3$ we could compute\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w_3}{\\partial w_1},\\nonumber \\\\\n",
    "\\frac{\\partial w_3}{\\partial w_2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Looking closely at the stepwise computational list\n",
    "\n",
    "1. $w_1  =  x^2$, $\\frac{\\partial w_1}{\\partial x}=2x$\n",
    "2. $w_2  =  y^2$, $\\frac{\\partial w_2}{\\partial y}=2y$\n",
    "3. $w_3  = w_1 + w_2$, $\\frac{\\partial w_3}{\\partial w_1}\\frac{\\partial w_1}{\\partial x} \n",
    "                       +\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial y}\n",
    "                       = 1\\cdot 2x e_x + 1\\cdot 2y e_y =2x+2y$\n",
    "                       \n",
    "we see that the last step involves a summation. If we start the computation at step 3 we first have\n",
    "to undo the summation, which in this case consists of computing the factors\n",
    "$\\frac{\\partial w_3}{\\partial w_1}$ and $\\frac{\\partial w_3}{\\partial w_2}$ which in our case\n",
    "both are equal to one. So our first step would be to compute a vector, $\\mathbf{u}$ as follows\n",
    "\n",
    "3. $\\mathbf{u}^3 = (u^3_0,u^3_1)=(\\frac{\\partial w_3}{\\partial w_1}, \\frac{\\partial w_3}{\\partial w_2}) = (1,1)$,\n",
    "since $w_3 = w_1 + w_2$.\n",
    "\n",
    "Remembering that the derivatives given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w_3}{\\partial x} & = & \\frac{\\partial w_3}{\\partial w_1}\\frac{\\partial w_1}{\\partial x},\n",
    "                                \\nonumber \\\\\n",
    "\\frac{\\partial w_3}{\\partial y} & = &\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial y}.\n",
    "                                \\label{eq:chain2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "we have now computed the first factors on the right hand side, so we evaluate now left to right which\n",
    "is the opposite order of the calculations above where we evaluated the factors right to left.\n",
    "\n",
    "We can now proceed to calculate the full derivative \n",
    "by\n",
    "\n",
    "2. $u^2 = u^3_0\\frac{\\partial w_2}{\\partial y} = 1\\cdot 2y$ \n",
    "\n",
    "and \n",
    "\n",
    "1. $u^1 = u^3_1\\frac{\\partial w_1}{\\partial x} = 1\\cdot 2x$\n",
    "\n",
    "The gradient is now given by \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} & = & u^2 = 2x\\nonumber,\\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial x} & = & u^1 = 2y \\nonumber. \n",
    "\\end{eqnarray}\n",
    "\n",
    "Doing the calculation in this **backward** order we actually end up with the components of the \n",
    "gradient and not the sum of the components as we did in the **forward** calculation.\n",
    "The list of computing steps for the backward method is then\n",
    "<ol reversed>\n",
    "<li >$\\mathbf{u}^3 = (u^3_0,u^3_1)=(\\frac{\\partial w_3}{\\partial w_1}, \\frac{\\partial w_3}{\\partial w_2}) = (1,1)$\n",
    "<li> $u^2 = u^3_0\\frac{\\partial w_2}{\\partial y} = 1\\cdot 2y$ \n",
    "<li> $u^1 = u^3_1\\frac{\\partial w_1}{\\partial x} = 1\\cdot 2x$\n",
    "</ol>\n",
    "\n",
    "Note that the backward method is *not* just reversal of steps of the forward method, it is a different calculation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ecda52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx: 2\n",
      "df/dy: 2\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "  return [x*x,2*x]\n",
    "\n",
    "\n",
    "def f3(w1,w2,e):\n",
    "    fval = w1[0]+w2[0]\n",
    "    dval = e[0]*w1[1]+e[1]*w2[1]\n",
    "    return [fval,dval]\n",
    "\n",
    "def df(x,e):\n",
    "  # Step 1\n",
    "  w1 = f1(x[0])\n",
    "  # Step 2\n",
    "  w2 = f1(x[1])\n",
    "  # Step 3\n",
    "  w3 = f3(w1,w1,e)\n",
    "  return w3\n",
    "\n",
    "def b3(w3) :\n",
    "  return [1,1]\n",
    "\n",
    "def b2(u3,x) :\n",
    "  return u3[0]*2*x[1]\n",
    "\n",
    "def b1(u3,x) :\n",
    "  return u3[1]*2*x[0]\n",
    "\n",
    "def dfb(w3,x):\n",
    "  #Step 3\n",
    "  u3 = b3(w3)\n",
    "  u2 = b2(u3,x)\n",
    "  u1 = b1(u3,x)\n",
    "  return [u1,u2]\n",
    "\n",
    "#Initial values for x\n",
    "x = [1,1]\n",
    "e = [1,1]\n",
    "\n",
    "#First run the forward calculation\n",
    "w3 = df(x,e)\n",
    "\n",
    "#Run the backward calculation\n",
    "u = dfb(w3,x)\n",
    "print(\"df/dx:\", u[0])\n",
    "print(\"df/dy:\", u[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77a14c",
   "metadata": {},
   "source": [
    "To do the calculation above in the backward mode we see that we need to first do the forward calculation and then compute the derivatives. The amount of computation is roughly two times the amount of computation in the forward calculation, which is the same as the two calculations required using the forward method.\n",
    "However, if the number of components in the gradient is large, the amount of computation is proportional to the number of components, while the backward method still requires only two forward calculations.\n",
    "\n",
    "##  Python libraries for automatic differentiation\n",
    "\n",
    "Automatic differentiation can be implemented as we have done above by manually creating computational steps and assigning differentiation rules. However, this is not feasible if the computation is very complicated. Fortunately it is possible to implement automatic differentiation in a fully automatic (no pun intended) way. The methodlogy is the same as described above but implemented by parsing expressions and building the derivative rules automatically.\n",
    "\n",
    "There are several python libraries who are capable of doing this, somewhat arbitrarily I use a library called Autograd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007835cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx:  [2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "\n",
    "def f(x) :\n",
    "    y = np.dot(x,x)\n",
    "    return y\n",
    "\n",
    "x=np.zeros(2)\n",
    "x[0] = 1.0\n",
    "x[1] = 1.0\n",
    "grad_f = grad(f)       # Obtain its gradient function\n",
    "df = grad_f(x)\n",
    "\n",
    "print(\"df/dx: \", df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0907a8d",
   "metadata": {},
   "source": [
    "## Optimization using automatic differentiation\n",
    "\n",
    "Now assume we want to solve an optimization problem as f.ex find\n",
    "the minimum of the function $f(x,y)$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(x.y) = (x-2)^2 + (y-1)^2\n",
    "\\end{eqnarray}\n",
    "We can solve the problem by using two python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5eaed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.500000\n",
      "         Iterations: 1\n",
      "         Function evaluations: 3\n",
      "         Gradient evaluations: 3\n",
      "x:  [1.5 1.5]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from scipy.optimize import fmin_cg as cg\n",
    "\n",
    "\n",
    "def f(x) :\n",
    "    y = np.dot(x-2,x-1)\n",
    "    return y\n",
    "\n",
    "def gradient(x):\n",
    "  grad_f = grad(f)       # Obtain its gradient function\n",
    "  df = grad_f(x)\n",
    "  return(df)\n",
    "\n",
    "x0=np.zeros(2)\n",
    "x0[0] = 0.0\n",
    "x0[1] = 0.0\n",
    "x=cg(f,x0,fprime=gradient)\n",
    "print(\"x: \",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd437a03",
   "metadata": {},
   "source": [
    "## Relation with adjoint \n",
    "\n",
    "For the simple example the error function $f$ is given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  f(x,y) = x^2+y^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "For a small change $\\Delta f$ we can write\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\Delta f(x,y) = \\frac{\\partial f(x,y)}{\\partial x}\\Delta x + \\frac{\\partial f(x,y)}{\\partial y}\\Delta y\n",
    "\\end{eqnarray}\n",
    "\n",
    "In matrix notation this is\n",
    "\\begin{eqnarray}\n",
    "\\begin{bmatrix}\n",
    "  \\Delta f \\\\\n",
    "  0        \n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} & \\frac{\\partial f(x,y)}{\\partial y} \\\\\n",
    "         0                                 &          0                 \\\\                                  \n",
    " \\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    " \\Delta x \\\\\n",
    " \\Delta y \\\\\n",
    " \\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "If we denote\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{J} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x} & \\frac{\\partial f(x,y)}{\\partial y} \\\\\n",
    "         0                                 &          0                 \\\\                                  \n",
    " \\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "and write the vectors\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f} = \n",
    "\\begin{bmatrix}\n",
    "\\Delta f \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{eqnarray},\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "\\Delta x \\\\\n",
    "\\Delta y\n",
    "\\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "We can write\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f} = \\mathbf{J}\\mathbf{x}\n",
    "                \\label{eq:J}\n",
    "\\end{eqnarray}\n",
    "\n",
    "If we now use $\\mathbf{J}^T$ to operate on $\\mathbf{f}$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{J}^T \\mathbf{f} = \\mathbf{J}^T\\mathbf{J}\\mathbf{x}\n",
    "                \\label{eq:Jt}\n",
    "\\end{eqnarray}\n",
    "\n",
    "we get on the left hand side\n",
    "\n",
    "\n",
    "Which gives\n",
    "\\begin{eqnarray}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(x,y)}{\\partial x}                   &    0           \\\\\n",
    "\\frac{\\partial f(x,y)}{\\partial y}                   &    0           \\\\                   \n",
    " \\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "  \\Delta f \\\\\n",
    "  0        \n",
    "\\end{bmatrix}=\n",
    " \\begin{bmatrix}\n",
    " \\frac{\\partial f(x,y)}{\\partial x}\\Delta f \\\\\n",
    " \\frac{\\partial f(x,y)}{\\partial y}\\Delta f \\\\\n",
    " \\end{bmatrix}\n",
    "\\end{eqnarray}\n",
    "\n",
    "If $\\Delta f=1$ we get the gradient components.\n",
    "$\\mathbf{J}^T$ is said to be adjoint of $\\mathbf{J}$.\n",
    "We see that using the adjoint of the jacobian to operate on the\n",
    "output $\\Delta f$ we get the gradient components.\n",
    "The backward method corresponds to using the adjoint of the Jacobian, while\n",
    "the forward method corresponds to using the Jacobian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed632415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
