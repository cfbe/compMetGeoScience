{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b30e378",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "### Carl Fredrik Berg, NTNU, 2023\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this note we will discuss optimization of a function when we know both the function and its gradient. For this we will consider the <i>gradient decent</i> method (aka. steepest descent).\n",
    "\n",
    "Let $F \\colon \\mathbb{R}^n \\to \\mathbb{R}$ be a multi-variable real-valued function. The main idea of the gradient descent method is to search for a minimum (or maximum) of $F(\\vec{x})$ in the opposite (or along the) direction the gradient is pointing. Remember that the gradient points in the direction of sharpest increase of the function $F$. By following the direction of decrease, a smaller value of $F$ can be found.\n",
    "\n",
    "The gradient descent method is usually attributed to the French mathematician Augustin-Louis Cauchy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89c983",
   "metadata": {},
   "source": [
    "## Gradient decent\n",
    "\n",
    "Let $F \\colon \\mathbb{R}^n \\to \\mathbb{R}$ be a multi-variable real-valued function such that $\\frac{\\partial F}{\\partial x_i}$ exist for all $i \\in \\{1,2,\\dots,n\\}$. Remember that the gradient of $F$ is \n",
    "$$\\nabla F(\\vec{x}) = \\left[ \\frac{\\partial F(\\vec{x})}{\\partial x_1}, \\frac{\\partial F(\\vec{x})}{\\partial x_2}, \\frac{\\partial F(\\vec{x})}{\\partial x_3}, \\dots, \\frac{\\partial F(\\vec{x})}{\\partial x_n} \\right]^T$$\n",
    "\n",
    "Let $\\vec{x}_0 \\in \\mathbb{R}^n$ be our starting location. For each iteration $i$ we will consider the negative gradient of $F$ at $\\vec{x}_i$, i.e., $- \\nabla F(\\vec{x}_i)$. The function $F$ will decrease fastest in the direction $ - \\nabla F(\\vec{x}_i)$, thus the common alternative name steepest decent. If we then take\n",
    "$$\\vec{x}_{i+1} = \\vec{x}_i - \\gamma_i \\nabla F(\\vec{x}_i)$$\n",
    "we know that $F(\\vec{x}_i) \\geq F(\\vec{x}_{i+1})$ if we take the step length $\\gamma_i$ small enough. Thus for an intelligent choice of $\\gamma_i$ values, we get a sequence $\\vec{x}_0, \\vec{x}_1, \\vec{x}_2, \\dots$ such that $F(\\vec{x}_0) \\geq F(\\vec{x}_1) \\geq F\\vec{x}_2) \\geq \\dots$, thus the sequence $\\{ \\vec{x}_i \\}$ should converge towards a local minimum.\n",
    "\n",
    "The step length $\\gamma_i$ can be chosen by different methods. Consider\n",
    "$$\\begin{equation}\n",
    "F(\\vec{x}_i - \\gamma_i \\nabla F(\\vec{x}_i))\n",
    "\\end{equation}$$\n",
    "as a one-dimensional function with respect to the variable $\\gamma_i$. Then one could do a minimization of this one-dimensional function, i.e., doing a line search. This can be done by several methods, including the bracket methods we have considered earlier. Note that if we ask for the exact minimum of this one-dimensional function, one such method will be the conjugate gradient method.\n",
    "\n",
    "We will first test the method by just using a random small step size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11abc0",
   "metadata": {},
   "source": [
    "### Example for single variable function\n",
    "\n",
    "We will first consider a simple example, namely the function $f(x) = x^3 - 2x$. This function always have a derivative, namely $f'(x) = 3x^2 - 2$. The extrema will be given by the points where the derivative is zero. Thus \n",
    "$$\\begin{align}\n",
    "f'(x) &= 0 \\\\\n",
    "3x^2 - 2 &= 0 \\\\\n",
    "x^2 &= \\frac{2}{3} \\\\\n",
    "x &= \\pm \\sqrt{\\frac{2}{3}}\n",
    "\\end{align}$$\n",
    "\n",
    "We will now try to use the gradient decent method to find the minimum value of the function. This is done in the python script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ad9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return x**3 - 2*x\n",
    "\n",
    "def grad(x):\n",
    "    return 3*x**2 -2\n",
    "\n",
    "#Maximum number of iterations\n",
    "n = 20\n",
    "\n",
    "#Error tolerance\n",
    "eps = 1e-4\n",
    "\n",
    "#Storing values for plotting\n",
    "xx=np.zeros(n)\n",
    "\n",
    "xx[0]=1.0\n",
    "diff=2*eps\n",
    "\n",
    "gamma=0.05\n",
    "\n",
    "ii=0\n",
    "while(ii<(n-1) and diff>eps):\n",
    "    xx[ii+1]=xx[ii]-gamma*(grad(xx[ii])/np.abs(grad(xx[ii])))\n",
    "    print(xx[ii+1])\n",
    "    diff=np.abs(xx[ii+1]-xx[ii])\n",
    "    ii+=1\n",
    "    \n",
    "print('Number of steps: ',ii)\n",
    "print('Solution: ',xx[ii])\n",
    "print('Exact solution: ',0,np.sqrt(2/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697aa73",
   "metadata": {},
   "source": [
    "### Example for multi-variable function\n",
    "\n",
    "In this example we will consider the Himmelblau's function $F(x,y) = \\left(x^2 + y - 11 \\right)^2 + \\left(x + y^2 - 7 \\right)^2$, and try to find the minimum by using the steepest decent with a constant pre-factor $\\gamma$ for the step size.\n",
    "\n",
    "We find the negative gradient as\n",
    "$$\\begin{align}\n",
    "- \\nabla F &= - \\left( \\frac{\\partial F}{\\partial x} , \\frac{\\partial F}{\\partial y} \\right) \\\\\n",
    "&= - \\left( 4 x\\left(x^2 + y - 11 \\right) + 2\\left(x + y^2 - 7 \\right)  , 2 \\left(x^2 + y - 11 \\right) + 4 y \\left(x + y^2 - 7 \\right) \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Then we find\n",
    "$$\\vec{x}_{i+1} = \\vec{x}_i - \\gamma \\nabla F(\\vec{x}_i)$$\n",
    "for a constant small value of $\\gamma$. This is implemented in python below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This first part is only included as a way of obtaining the gradient. You could just use the derivatives above.\n",
    "import sympy as sym\n",
    "x, y = sym.symbols('x y')\n",
    "xd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,x)\n",
    "yd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,y)\n",
    "#print(xd)\n",
    "#print(yd)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Func(x,y):\n",
    "    return (x**2+y-11)**2+(x+y**2-7)**2\n",
    "\n",
    "def Grad(x,y):\n",
    "    return np.array([(4*x*(x**2 + y - 11) + 2*x + 2*y**2 - 14),(2*x**2 + 4*y*(x + y**2 - 7) + 2*y - 22)])\n",
    "\n",
    "#Max number of steps\n",
    "nn=1000\n",
    "#Saving the intermediate steps to be able to plot the path.\n",
    "xvec=np.zeros((nn,2))\n",
    "#The initial point\n",
    "xvec[0,:]=[-2,2]\n",
    "\n",
    "#Constant for step length\n",
    "gamma=0.001\n",
    "#Convergence criteria\n",
    "eps=1E-6\n",
    "diff=2*eps\n",
    "ii=0\n",
    "while ii<nn and diff>eps:\n",
    "    ii+=1\n",
    "    xvec[ii,:]=xvec[ii-1,:]-gamma*Grad(xvec[ii-1,0],xvec[ii-1,1])\n",
    "    diff=np.sqrt(np.sum((xvec[ii,:]-xvec[ii-1,:])**2))\n",
    "    \n",
    "print(\"Number of steps: \",ii)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.copy(x)\n",
    "\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "xr=np.ravel(xv)\n",
    "yr=np.ravel(yv)\n",
    "\n",
    "z=[]\n",
    "for jj in range(0,len(xr)):\n",
    "    z.append(Func(xr[jj],yr[jj]))\n",
    "zr=np.asarray(z)\n",
    "zv=zr.reshape(xv.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contour(xv,yv,zv,100)\n",
    "plt.plot(xvec[:ii,0],xvec[:ii,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ccdf6",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "<i>Newton's method</i> (aka. Newtonâ€“Raphson method) is a numerical method to find the root of a function. The method is using the derivative of the function. \n",
    "\n",
    "The main idea is to use the tangent line as an approximation of the function, and then find the root of this approximate function. The approximate root of the tangent is expected to bring one iteratively closer to the root of the function.\n",
    "\n",
    "The method works for multi-dimensional functions, however, as we will only need a special version of it later, we will restrict ourselves to only considering a real-valued function. The extension to multi-dimensional functions is straight forward.\n",
    "\n",
    "Let $f \\colon \\mathbb{R} \\to \\mathbb{R}$ be a real-valued function with derivative $f'$, and let $x_0$ be an initial guess. For our iterative procedure, assume we are at point $x_i$ after $i$ iterations. Then we approximate the function $f$ by the linear function $f(x_i)+(x-x_i) f'(x_i)$, which gives the tangent of $f$ in point $x_i$. If we take the root of this linear function as $x_{i+1}$, we get\n",
    "$$\\begin{align}\n",
    "f(x_i)+(x_{i+1}-x_i) f'(x_i) &= 0 \\\\\n",
    "x_{i+1}  &= x_i - \\frac{f(x_i)}{f'(x_i)} \n",
    "\\end{align}$$\n",
    "\n",
    "Iterating we are expected to get close to the root of $f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4314fdc",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "Let us consider the simple example $f(x) = x(x^2-2) = x^3-2x$. We immediately see that the roots are $x = \\pm \\sqrt{2}$ and $x = 0$. Let us try to solve this using a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x**3 - 2*x\n",
    "\n",
    "def grad(x):\n",
    "    return 3*x**2 -2\n",
    "\n",
    "\n",
    "#Create an intervall x to plot f(x)\n",
    "xv = np.linspace(-1,2,100)\n",
    "\n",
    "#Maximum number of iterations\n",
    "n = 10\n",
    "nn = np.linspace(0,n,n)\n",
    "\n",
    "#Error tolerance\n",
    "eps = 1e-4\n",
    "\n",
    "#Storing values for plotting\n",
    "xx=np.zeros(n)\n",
    "\n",
    "xx[0]=1.0\n",
    "diff=2*eps\n",
    "fig = plt.figure()\n",
    "plt.plot(xv,func(xv),color='k')\n",
    "plt.plot((-1,2),(0,0),color='k',linestyle='dashed')\n",
    "\n",
    "ii=0\n",
    "while(ii<(n-1) and diff>eps):\n",
    "    xx[ii+1]=xx[ii]-func(xx[ii])/grad(xx[ii])\n",
    "    plt.plot((xx[ii],xx[ii+1]),(func(xx[ii]),0),color='r')\n",
    "    diff=np.abs(xx[ii+1]-xx[ii])\n",
    "    ii+=1\n",
    "    \n",
    "print('Number of steps: ',ii)\n",
    "print('Solution: ',xx[ii])\n",
    "print('Exact solution: ',0,np.sqrt(2))\n",
    "print(xx)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cad3bf",
   "metadata": {},
   "source": [
    "### Newton's method in optimization\n",
    "\n",
    "As described above, Newton's method is a numerical method to find the roots of a function, i.e., points $x$ where $f(x) = 0$.\n",
    "\n",
    "To find the extrema of a function, we want to find points where $f'(x)=0$. If the function $f$ is twice differentiable, we can use Newton's method to the derivative $f'$ to find the roots of $f'$, i.e., the points where $f'(x) = 0$.\n",
    "\n",
    "Applying Newton's method to $f'$, we have the following iterative algorithm:\n",
    "\n",
    "$$x_{i+1} = x_i - \\frac{f'(x_i)}{f''(x_i)}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "We will continue the example above, with the function $f(x) = x(x^2-2) = x^3-2x$. Then the derivative is $f'(x) = 3x^2-2$, which will give extrema at \n",
    "$$x = \\pm \\sqrt{\\frac{2}{3}}$$\n",
    "\n",
    "This is implemented in the python code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad2(x):\n",
    "    return 6*x\n",
    "\n",
    "#Maximum number of iterations\n",
    "n = 10\n",
    "nn = np.linspace(0,n,n)\n",
    "\n",
    "#Error tolerance\n",
    "eps = 1e-4\n",
    "\n",
    "#Storing values for plotting\n",
    "xx=np.zeros(n)\n",
    "\n",
    "xx[0]=-0.5\n",
    "diff=2*eps\n",
    "fig = plt.figure()\n",
    "plt.plot(xv,func(xv),color='k')\n",
    "plt.plot(xv,grad(xv),color='k',linestyle='dashed')\n",
    "plt.plot((-1,2),(0,0),color='k',linestyle='dotted')\n",
    "\n",
    "\n",
    "ii=0\n",
    "while(ii<(n-1) and diff>eps):\n",
    "    xx[ii+1]=xx[ii]-grad(xx[ii])/grad2(xx[ii])\n",
    "    plt.plot((xx[ii],xx[ii+1]),(grad(xx[ii]),0),color='r')\n",
    "    diff=np.abs(xx[ii+1]-xx[ii])\n",
    "    ii+=1\n",
    "    \n",
    "print('Number of steps: ',ii)\n",
    "print('Solution: ',xx[ii])\n",
    "print('Exact solution: ',np.sqrt(2/3))\n",
    "print(xx)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57daf22c",
   "metadata": {},
   "source": [
    "## Newton's method in higher dimensions\n",
    "\n",
    "We now want to apply the Newton's method in higher dimensions. Let $F \\colon \\mathbb{R}^n \\to \\mathbb{R}$ be a multi-variable real-valued function such that $\\frac{\\partial F}{\\partial x_i}$ and $\\frac{\\partial^2 F}{\\partial x_i \\partial x_j}$ exist for all $i,j \\in \\{1,2,\\dots,n\\}$. Remember that the gradient of $F$ is \n",
    "$$\\nabla F(\\vec{x}) = \\left[ \\frac{\\partial F(\\vec{x})}{\\partial x_1}, \\frac{\\partial F(\\vec{x})}{\\partial x_2}, \\frac{\\partial F(\\vec{x})}{\\partial x_3}, \\dots, \\frac{\\partial F(\\vec{x})}{\\partial x_n} \\right]^T$$\n",
    "\n",
    "The Taylor expansion of the function $F$ can be written as\n",
    "$$F(\\vec{x} + \\Delta \\vec{x}) \\simeq F(\\vec{x}) + \\nabla F(\\vec{x})^T\\Delta \\vec{x} + \\frac{1}{2} \\Delta \\vec{x}^T \\mathbf{H}_F(\\vec{x}) \\Delta \\vec{x}$$\n",
    "where $\\mathbf{H}$ is the Hessian matrix defined as\n",
    "$$\\mathbf{H}_F(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 F(\\vec{x})}{\\partial x_1^2} & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_1 \\partial x_n}  \\\\\n",
    "\\frac{\\partial^2 F(\\vec{x})}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_2 \\partial x_n}  \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial^2 F(\\vec{x})}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 F(\\vec{x})}{\\partial x_n^2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The Hessian matrix of $F$ is the Jacobian matrix of the gradient of the function $F$, i.e., $\\mathbf{H}_F = \\mathbf{J}(\\nabla F(\\vec{x}))$.\n",
    "\n",
    "The iterative procedure is then similar to what we had before, just using the multi-variable versions of the derivative and second order derivative, i.e., the gradient and the Hessian matrix:\n",
    "\n",
    "$$\\vec{x}_{i+1} = \\vec{x}_i - \\nabla F(\\vec{x}_i) \\mathbf{H}_F^{-1}(\\vec{x}_i)$$\n",
    "\n",
    "Note that this method involves a matrix inversion.\n",
    "\n",
    "It is also common to add a variable $\\gamma$ to control the step size, as:\n",
    "\n",
    "$$\\vec{x}_{i+1} = \\vec{x}_i - \\gamma \\nabla F(\\vec{x}_i) \\mathbf{H}_F^{-1}(\\vec{x}_i)$$\n",
    "\n",
    "An example use is shown in the python code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049720d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This first part is only included as a way of obtaining the Hessian.\n",
    "import sympy as sym\n",
    "x, y = sym.symbols('x y')\n",
    "\n",
    "xdd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,x,x)\n",
    "xdyd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,x,y)\n",
    "ydxd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,y,x)\n",
    "ydd=sym.diff((x**2+y-11)**2+(x+y**2-7)**2,y,y)\n",
    "\n",
    "#print(xdd)\n",
    "#print(xdyd)\n",
    "#print(ydxd)\n",
    "#print(ydd)\n",
    "\n",
    "def Hessian(x,y):\n",
    "    return np.array([[2*(6*x**2 + 2*y - 21),4*(x + y)],[4*(x + y),2*(2*x + 6*y**2 - 13)]])\n",
    "\n",
    "#Max number of steps\n",
    "nn=1000\n",
    "#Saving the intermediate steps to be able to plot the path.\n",
    "xvec=np.zeros((nn,2))\n",
    "#The initial point\n",
    "xvec[0,:]=[2,1]\n",
    "\n",
    "#Constant for step length\n",
    "gamma=1.0\n",
    "#Convergence criteria\n",
    "eps=1E-6\n",
    "diff=2*eps\n",
    "ii=0\n",
    "while ii<nn and diff>eps:\n",
    "    ii+=1\n",
    "    invHessian=np.linalg.inv(Hessian(xvec[ii-1,0],xvec[ii-1,1]))\n",
    "    xvec[ii,:]=xvec[ii-1,:]-gamma*np.matmul(Grad(xvec[ii-1,0],xvec[ii-1,1]),invHessian)\n",
    "    diff=np.sqrt(np.sum((xvec[ii,:]-xvec[ii-1,:])**2))\n",
    "\n",
    "    \n",
    "print(\"Number of steps: \",ii)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contour(xv,yv,zv,100)\n",
    "plt.plot(xvec[:ii,0],xvec[:ii,1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
