{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fde129",
   "metadata": {},
   "source": [
    "## Decision tree classifier\n",
    "\n",
    "A decision tree classifier attempts to group a set of observations into classes based on logical criteria.\n",
    "An example of observations and classes is the table below\n",
    "\n",
    "![Data table](dt-ex1.png)\n",
    "\n",
    "The last column (Play?) describes the two classes, yes or no (for playing tennis).\n",
    "The other columns are weather observations which are used to decide wether tennis could be played or not.\n",
    "We can use a tree structure to systematically check the observations in order to arrive at a decision.\n",
    "The figure below shows an example using the table above\n",
    "\n",
    "![Decision Tree](dt-ex2.png)\n",
    "\n",
    "The process of constructing the tree can of course be done manually, but we are looking for an automated process. One way of doing that is first look at the entire dataset and try to find out to what extent\n",
    "decisison can be made with respect to how it can be split up into classes. Several methods have been proposed,\n",
    "one of them is to use the statistical Gini index.\n",
    "Assume we have n classes and a single observation. The probability for a random observation to be put into\n",
    "class no $i$ is set to $p_i$. The probability for a random variable to not belong to class no $i$ is \n",
    "$1-p_i$. The combined probability for a random observation to be put into class no $i$ and not belong to that class is $p_i(1-p_i)$. The Gini index G is then found by summing over all classes:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "G = \\sum_{i=1}^n p_i(1-p_i) = \\sum_{i=1}^n p_i -\\sum_{i=1}^n p^2_i = 1-\\sum_{i=1}^n p^2_i.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now let us look at a slightly simpler example. Below is a table containing the observations \"Humidity\" and\n",
    "\"Wind\". The classes to be determined from the observations are \"Sunny\" and \"Rain\".\n",
    "\n",
    "![Weather observations](dt-rain.png)\n",
    "\n",
    "The Gini index for the table above could the be calculated as:\n",
    "\n",
    " 1. Probability that an observation belongs to the rain class : $p_1 = 6/10$.\n",
    " 2. Probability that an observation belongs to the sunny class: $p_2 = 4/10$.\n",
    "\n",
    "The Gini index is then\n",
    "\n",
    "\\begin{eqnarray}\n",
    "G = 1-\\left(6/10)\\right)^2 - \\left(4/10\\right)^2 = 0.48\n",
    "\\end{eqnarray}\n",
    "\n",
    "The main idea of the tree algorithm is to split the dataset into two separate datasets. For each\n",
    "dataset the Gini index is computed. A combined Gini index is computed by an average of the two Gini indexes weigthed by the number of observations in each dataset.\n",
    "For the split shown in the figure below the Gini index for the right dataset, with only one member, is zero and the Gini index for the left dataset is 0.44 resulting in an average Gini index of 0.44\n",
    "This is lower than the Gini index for the total dataset (0.48), so this is a potential possible split.\n",
    "The Gini index and the value of the wind parameter is then recorded as the (for now) best Gini index and\n",
    "the threshold value.\n",
    "The algorithm then proceeds to evaluate the other observations one by one and the observation resulting\n",
    "in the best Gini index is kept, together with the value of the observation.\n",
    "The algorithm then proceeds by evaluating all possible splits of the dataset, keeping in the end the split with the lowest Gini index.\n",
    "\n",
    "The whole process is repeated for each of the two datasets created by the split, eventually resulting in a\n",
    "complete tree structure where each node have an observation and a threshold value. \n",
    "\n",
    "The tree can be used to decide wether it was rainy or sunny on a given day by evaluating the condition\n",
    "in each node.\n",
    "\n",
    "\n",
    "\n",
    "![Weather observations](dt-split.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe01295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# Load the data table\n",
    "n=10\n",
    "m=2\n",
    "X = np.zeros((n,m))\n",
    "Y = np.zeros(n)\n",
    "\n",
    "# Classes\n",
    "rainy=0\n",
    "sunny=1\n",
    "\n",
    "# Humidity\n",
    "X[0,0] = 5.1\n",
    "X[1,0] = 4.7\n",
    "X[2,0] = 4.6\n",
    "X[3,0] = 5.0\n",
    "X[4,0] = 3.4\n",
    "X[5,0] = 1.5\n",
    "X[6,0] = 1.6\n",
    "X[7,0] = 1.5\n",
    "X[8,0] = 3.9\n",
    "X[9,0] = 1.5\n",
    "\n",
    "# Wind\n",
    "X[0,1] = 3.5\n",
    "X[1,1] = 3.2\n",
    "X[2,1] = 1.5\n",
    "X[3,1] = 3.6\n",
    "X[4,1] = 0.2\n",
    "X[5,1] = 0.1\n",
    "X[6,1] = 0.2\n",
    "X[7,1] = 0.4\n",
    "X[8,1] = 0.4\n",
    "X[9,1] = 0.2\n",
    "\n",
    "#Class\n",
    "Y[0] = rainy\n",
    "Y[1] = sunny\n",
    "Y[2] = rainy\n",
    "Y[3] = sunny\n",
    "Y[4] = rainy\n",
    "Y[5] = rainy\n",
    "Y[6] = sunny\n",
    "Y[7] = rainy\n",
    "Y[8] = rainy\n",
    "Y[9] = sunny\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "#tree.plot_tree(clf)\n",
    "\n",
    "clf.predict([[5.0, 3.6]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
